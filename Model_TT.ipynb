{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras with image MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pylibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIF = 0\n",
    "DDA = 1\n",
    "#data_augmentation\n",
    "def data_augmentation(image):\n",
    "    intensityL1 = 1.2\n",
    "    intensityL2 = 1.0\n",
    "    \n",
    "    new_image = np.zeros_like(image)\n",
    "    # Resize image to a random size between 27 and 28\n",
    "    new_size = np.random.randint(26, 28)\n",
    "    image = zoom(image, (new_size / image.shape[0], new_size / image.shape[1], 1))\n",
    "    pad_size = (np.random.randint(1, 7), np.random.randint(1, 7))\n",
    "    # Choose a random side or corner to pad from\n",
    "    side_or_corner = np.random.choice(['top', 'bottom', 'left', 'right', 'top-left', 'top-right', 'bottom-left', 'bottom-right', 'AD'])\n",
    "\n",
    "    # Set the padding size based on the chosen side or corner\n",
    "    if side_or_corner == 'top':\n",
    "        pad_size = ((pad_size[0], 0), (0, 0), (0, 0))\n",
    "    elif side_or_corner == 'bottom':\n",
    "        pad_size = ((0, pad_size[0]), (0, 0), (0, 0))\n",
    "    elif side_or_corner == 'left':\n",
    "        pad_size = ((0, 0), (pad_size[1], 0), (0, 0))\n",
    "    elif side_or_corner == 'right':\n",
    "        pad_size = ((0, 0), (0, pad_size[1]), (0, 0))\n",
    "    elif side_or_corner == 'top-left':\n",
    "        pad_size = ((pad_size[0], 0), (pad_size[1], 0), (0, 0))\n",
    "    elif side_or_corner == 'top-right':\n",
    "        pad_size = ((pad_size[0], 0), (0, pad_size[1]), (0, 0))\n",
    "    elif side_or_corner == 'bottom-left':\n",
    "        pad_size = ((0, pad_size[0]), (pad_size[1], 0), (0, 0))\n",
    "    elif side_or_corner == 'bottom-right':\n",
    "        pad_size = ((0, pad_size[0]), (0, pad_size[1]), (0, 0))\n",
    "    else: #'AD'\n",
    "        pad_size = ((pad_size[0], pad_size[0]), (pad_size[1], pad_size[1]), (0, 0))\n",
    "        \n",
    "    padded_image = np.pad(image, pad_size, mode='constant')\n",
    "\n",
    "    new_image = padded_image\n",
    "    image = new_image\n",
    "    \n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    image = resized_image[:, :, np.newaxis]\n",
    "    new_image = image\n",
    "    \n",
    "    crop_size = np.random.randint(23, 28)\n",
    "    # Choose a random direction\n",
    "    direction = np.random.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "    # Set the starting position of the cropped image based on the chosen direction\n",
    "    if direction == 'up':\n",
    "        start_x = np.random.randint(0, image.shape[1] - crop_size)\n",
    "        start_y = 0\n",
    "    elif direction == 'down':\n",
    "        start_x = np.random.randint(0, image.shape[1] - crop_size)\n",
    "        start_y = image.shape[0] - crop_size\n",
    "    elif direction == 'left':\n",
    "        start_x = 0\n",
    "        start_y = np.random.randint(0, image.shape[0] - crop_size)\n",
    "    else:  # 'right'\n",
    "        start_x = image.shape[1] - crop_size\n",
    "        start_y = np.random.randint(0, image.shape[0] - crop_size)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[start_y:start_y+crop_size, start_x:start_x+crop_size]\n",
    "\n",
    "    image = cropped_image\n",
    "    new_image = image\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    image = resized_image[:, :, np.newaxis]\n",
    "    new_image = image\n",
    "    for i in range(0, image.shape[0], 4):\n",
    "        for j in range(0, image.shape[1], 4):\n",
    "            block = image[i:i+4, j:j+4]\n",
    "            block = (np.random.rand() * intensityL2 + 1) * block\n",
    "            new_image[i:i+4, j:j+4] = block\n",
    "    image = new_image       \n",
    "    for i in range(0, image.shape[0], 2):\n",
    "        for j in range(0, image.shape[1], 2):\n",
    "            block = image[i:i+2, j:j+2]\n",
    "            block = (np.random.rand() * intensityL1 + 1) * block\n",
    "            new_image[i:i+2, j:j+2] = block\n",
    "    return new_image\n",
    "#save_images_to_dir\n",
    "def save_images_to_dir(images, labels, dir_path):\n",
    "    # create the directory if it doesn't exist\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    # iterate over the images and labels\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # get the class label\n",
    "        class_label = np.argmax(label)\n",
    "        # create the file path\n",
    "        file_path = os.path.join(dir_path, f'image_{i}_class_{class_label}.png')\n",
    "        # save the image to the file path\n",
    "        plt.imsave(file_path, image.squeeze(), cmap='gray')\n",
    "\n",
    "# load dataset\n",
    "with np.load('mnist.npz') as data:\n",
    "    x_train, y_train = data['x_train'], data['y_train']\n",
    "    x_test, y_test = data['x_test'], data['y_test']\n",
    "\n",
    "# reshape dataset to have a single channel\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# one hot encode target values\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# data processing\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# create a data generator\n",
    "datagen = ImageDataGenerator(rotation_range=90, horizontal_flip=True, preprocessing_function=lambda x: data_augmentation(1-x))\n",
    "\n",
    "# prepare an iterators to scale images\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=len(x_train))\n",
    "test_iterator = datagen.flow(x_test, y_test, batch_size=len(x_test))\n",
    "\n",
    "# get augmented data\n",
    "x_train_augmented, y_train_augmented = train_iterator.next()\n",
    "x_test_augmented, y_test_augmented = test_iterator.next()\n",
    "\n",
    "# append augmented data to original data\n",
    "x_train = np.concatenate([x_train, x_train_augmented])\n",
    "y_train = np.concatenate([y_train, y_train_augmented])\n",
    "x_test = np.concatenate([x_test, x_test_augmented])\n",
    "y_test = np.concatenate([y_test, y_test_augmented])\n",
    "\n",
    "if DDA:\n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(x_train, y_train, batch_size=len(x_train))\n",
    "    test_iterator = datagen.flow(x_test, y_test, batch_size=len(x_test))\n",
    "\n",
    "    # get augmented data\n",
    "    x_train_augmented, y_train_augmented = train_iterator.next()\n",
    "    x_test_augmented, y_test_augmented = test_iterator.next()\n",
    "\n",
    "    # append augmented data to original data\n",
    "    x_train = np.concatenate([x_train, x_train_augmented])\n",
    "    y_train = np.concatenate([y_train, y_train_augmented])\n",
    "    x_test = np.concatenate([x_test, x_test_augmented])\n",
    "    y_test = np.concatenate([y_test, y_test_augmented])\n",
    "    \n",
    "    # prepare an iterators to scale images\n",
    "    train_iterator = datagen.flow(x_train, y_train, batch_size=len(x_train))\n",
    "    test_iterator = datagen.flow(x_test, y_test, batch_size=len(x_test))\n",
    "\n",
    "    # get augmented data\n",
    "    x_train_augmented, y_train_augmented = train_iterator.next()\n",
    "    x_test_augmented, y_test_augmented = test_iterator.next()\n",
    "\n",
    "    # append augmented data to original data\n",
    "    x_train = np.concatenate([x_train, x_train_augmented])\n",
    "    y_train = np.concatenate([y_train, y_train_augmented])\n",
    "    x_test = np.concatenate([x_test, x_test_augmented])\n",
    "    y_test = np.concatenate([y_test, y_test_augmented])\n",
    "\n",
    "# save augmented data to directories\n",
    "if SIF:\n",
    "    #save_images_to_dir(x_train_augmented, y_train_augmented, 'train')\n",
    "    save_images_to_dir(x_test_augmented, y_test_augmented, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating a model...\\n')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (6, 6), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(256, (4, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (4, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(512, (4, 4), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(learning_rate=0.05, momentum=0.5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print('\\nThe model was successfully created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = load_model('MNIST_model.h5')\n",
    "except (ImportError, IOError) as e:\n",
    "    print(f'\\033[91mfailed to load the model ERROR:\\n{e}')\n",
    "else:\n",
    "    print('\\033[92m loading model done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights = True)\n",
    "\n",
    "print('Training the model...\\n')\n",
    "history = model.fit(x_train, y_train, epochs=128, batch_size=1, validation_data=(x_test, y_test), verbose='auto', callbacks=[early_stopping])\n",
    "print('Training done.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MNIST_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    try:\n",
    "        plt.plot(history.history['val_loss'], label='val_loss', color='orange')\n",
    "    except (ValueError, NameError):\n",
    "        print('\\033[91mfailed to load val_loss.')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "except (ValueError, NameError):\n",
    "    print('\\033[91mfailed to load model history.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    model = load_model('MNIST_model.h5')\n",
    "except (ImportError, IOError) as e:\n",
    "    print(f'failed to load the model ERROR:\\n{e}')\n",
    "    sys.exit()\n",
    "else:\n",
    "    print('loading model done.')\n",
    "\n",
    "# Load and preprocess image\n",
    "image_dir = input('image dir(.png/.jpg):')\n",
    "if image_dir == '':\n",
    "    image_dir = 'image.jpg'\n",
    "try:\n",
    "    img = Image.open(image_dir).convert('L')\n",
    "    if img.format == 'JPEG':\n",
    "        img = img.convert('RGB')\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        img = Image.open(image_dir).convert('L')\n",
    "        if img.format == 'JPEG':\n",
    "            img = img.convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print('cant find: image.(png/jpg)')\n",
    "        sys.exit()\n",
    "    \n",
    "img = img.resize((28, 28))\n",
    "img_array = np.array(img)\n",
    "img_array = img_array.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(img_array)\n",
    "predicted_digit = np.argmax(predictions)\n",
    "confidence = predictions[0][predicted_digit]\n",
    "print(f'The predicted digit is {predicted_digit} with a confidence of {confidence:.2f}')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists('IMG_ITER'):\n",
    "    os.mkdir('IMG_ITER')\n",
    "if not os.path.exists('IMG_ITER/white'):\n",
    "    os.mkdir('IMG_ITER/white')\n",
    "if not os.path.exists('IMG_ITER/black'):\n",
    "    os.mkdir('IMG_ITER/black')\n",
    "\n",
    "# Apply threshold and save images\n",
    "thresholds = range(20, 250, 5)\n",
    "white_confidences = []\n",
    "black_confidences = []\n",
    "inverted_white_confidences = []\n",
    "inverted_black_confidences = []\n",
    "digit_counts = np.zeros(10)\n",
    "digit_confidences = np.zeros(10)\n",
    "for threshold in thresholds:\n",
    "    W_threshold = threshold if threshold <= 195 else 195\n",
    "    white_thresholded_img = img.point(lambda x: 255 if x < W_threshold else x)\n",
    "    white_thresholded_img.save(f'IMG_ITER/white/threshold_{W_threshold}.png')\n",
    "    white_thresholded_img_array = np.array(white_thresholded_img)\n",
    "    white_thresholded_img_array = white_thresholded_img_array.reshape(1, 28, 28, 1)\n",
    "    predictions = model.predict(white_thresholded_img_array)\n",
    "    predicted_digit = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_digit]\n",
    "    white_confidences.append(confidence)\n",
    "    digit_counts[predicted_digit] += 1\n",
    "    digit_confidences[predicted_digit] += confidence\n",
    "    print(f'Threshold: {W_threshold}, Color: white, Predicted digit: {predicted_digit}, Confidence: {confidence:.2f}')\n",
    "    \n",
    "    black_thresholded_img = img.point(lambda x: 0 if x < threshold else x)\n",
    "    black_thresholded_img.save(f'IMG_ITER/black/threshold_{threshold}.png')\n",
    "    black_thresholded_img_array = np.array(black_thresholded_img)\n",
    "    black_thresholded_img_array = black_thresholded_img_array.reshape(1, 28, 28, 1)\n",
    "    predictions = model.predict(black_thresholded_img_array)\n",
    "    predicted_digit = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_digit]\n",
    "    black_confidences.append(confidence)\n",
    "    digit_counts[predicted_digit] += 1\n",
    "    digit_confidences[predicted_digit] += confidence\n",
    "    print(f'Threshold: {threshold}, Color: black, Predicted digit: {predicted_digit}, Confidence: {confidence:.2f}')\n",
    "\n",
    "    inverted_white_thresholded_img = img.point(lambda x: x if x < W_threshold else 255 - x)\n",
    "    inverted_white_thresholded_img_array = np.array(inverted_white_thresholded_img)\n",
    "    inverted_white_thresholded_img_array = inverted_white_thresholded_img_array.reshape(1, 28, 28, 1)\n",
    "    predictions = model.predict(inverted_white_thresholded_img_array)\n",
    "    predicted_digit = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_digit]\n",
    "    inverted_white_confidences.append(confidence)\n",
    "    digit_counts[predicted_digit] += 1\n",
    "    digit_confidences[predicted_digit] += confidence\n",
    "\n",
    "    inverted_black_thresholded_img = img.point(lambda x: x if x < threshold else 255 - x)\n",
    "    inverted_black_thresholded_img_array = np.array(inverted_black_thresholded_img)\n",
    "    inverted_black_thresholded_img_array = inverted_black_thresholded_img_array.reshape(1, 28, 28, 1)\n",
    "    predictions = model.predict(inverted_black_thresholded_img_array)\n",
    "    predicted_digit = np.argmax(predictions)\n",
    "    confidence = predictions[0][predicted_digit]\n",
    "    inverted_black_confidences.append(confidence)\n",
    "    digit_counts[predicted_digit] += 1\n",
    "    digit_confidences[predicted_digit] += confidence\n",
    "# Create graph\n",
    "plt.plot(thresholds, white_confidences, label='White')\n",
    "plt.plot(thresholds, black_confidences, label='Black')\n",
    "plt.plot(thresholds, inverted_white_confidences, label='Inverted White')\n",
    "plt.plot(thresholds, inverted_black_confidences, label='Inverted Black')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Confidence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display most predicted digits and their average confidence\n",
    "digit_counts = digit_counts / np.sum(digit_counts)\n",
    "digit_scores = np.zeros(10)\n",
    "for digit in range(10):\n",
    "    if digit_counts[digit] > 0:\n",
    "        avg_confidence = digit_confidences[digit] / ((digit_counts[digit] * len(thresholds) * 2) * 2)\n",
    "        print(f'Digit: {digit}, Percentage: {digit_counts[digit]:.2f}, Average Confidence: {avg_confidence:.2f}')\n",
    "        digit_scores[digit] = digit_counts[digit] + (avg_confidence / 3)\n",
    "\n",
    "best_digit = np.argmax(digit_scores)\n",
    "best_score = digit_scores[best_digit]\n",
    "print(f'\\nThe best result is for digit {best_digit} with a score of {best_score:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
